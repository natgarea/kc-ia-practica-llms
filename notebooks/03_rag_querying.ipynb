{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78698a17",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este notebook vamos a:\n",
    "* Cargar el índice creado en `02_indexing.ipynb` (vectorstore + metadatos).\n",
    "* Recuperar chunks relevantes (retrieval) para una pregunta del usuario usando búsqueda semántica.\n",
    "* Aplicar una regla simple anti-alucinación: si no hay evidencia suficiente, no recomendar y pedir aclaración.\n",
    "* Generar una respuesta con un modelo *instruct* usando únicamente el texto recuperado (grounded answer).\n",
    "* Incluir citas (snippets + URLs) para justificar las recomendaciones.\n",
    "\n",
    "Usaremos LlamaIndex para el retrieval y la generación, y el vectorstore persistente en Chroma como base de conocimiento.\n",
    "\n",
    "## Documentos de referencia\n",
    "* [Persisting & Loading Data | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/module_guides/storing/save_load/)\n",
    "* [Retriever | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/module_guides/querying/retriever/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ca86d-5f25-4152-a5f9-f7fc14eb00d4",
   "metadata": {},
   "source": [
    "## Imports\n",
    "* Importamos `chromadb`y `ChromaVectorStore` para reconectar con la DB persistente.\n",
    "* De `llama_index` nos traemos:\n",
    "    * [StorageContext](https://developers.llamaindex.ai/python/framework-api-reference/storage/storage_context/), `load_index_from_storage` porque hacen falta para cargar el índice.\n",
    "    * [Settings](https://developers.llamaindex.ai/python/framework/module_guides/supporting_modules/settings/) para fijar una configuración global de embeddings y LLM que vamos a usar.\n",
    "    * [HuggingFaceEmbeddings](https://developers.llamaindex.ai/typescript/framework/modules/models/embeddings/huggingface/) para continuar usando el embedding model `all-MiniLM-L6-v2`\n",
    "    * [HuggingFaceLLM](https://developers.llamaindex.ai/python/framework/integrations/llm/huggingface/) para usar un LLM que nos genere la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b16af36-f57e-4bc2-aa54-d7f42f1e4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a313ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configurar embeddings\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f8d2c-66de-486a-a1b0-52b1196d234a",
   "metadata": {},
   "source": [
    "## LLM\n",
    "Para esta tarea, necesitamos un modelo _instruct_ que siga unas instrucciones para generar las respuestas a los usuarios basadas en texto recuperado (RAG). También le podemos indicar que no de respuestas sin datos. [`phi-3-mini-4k-instruct`](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) es pequeño, se puede ejecutar en local y es suficiente para la tarea. Con 4k de tokens de contexto tenemos suficiente para las instrucciones, prompts, chunks recuperados y generar una respuesta corta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13eebf85-6ca2-4c15-bfff-34e1415517c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c798cb9c13243f8b9b3df224ab2f276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "# Configurar LLM\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/phi-3-mini-4k-instruct\",\n",
    "    tokenizer_name=\"microsoft/phi-3-mini-4k-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=300,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959c1778-8f24-4652-b09d-64699d669b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"charities\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"../data/index_store\",\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e978fd-fc00-4041-a2fe-f6e0662e386a",
   "metadata": {},
   "source": [
    "## Query\n",
    "Existen varios módulos de `Query` en LlamaIndex. En este caso nos interesa usar `Retriever` porque así podemos ver si hay chunks relacionados con la pregunta del usuario y evitar responder algo sin datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f52e0b-101d-4765-a884-36b3c24d9d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Recommended charities\n",
      "A) Ultra-Poverty Graduation Programs Multi-Component\n",
      "B) Raising The Village\n",
      "C) Ultra-Poor Entrepreneurship Programs\n",
      "\n",
      "2) Why\n",
      "A) Asset transfer plus support, sustainable livelihoods, women-led.\n",
      "B) Livelihood development, health, savings groups, education support.\n",
      "C) Business skills, seed capital, mentorship, sustainable micro-enterprises.\n",
      "\n",
      "3) Transparency notes\n",
      "A) Collects detailed household data, multiple RCTs.\n",
      "B) Publishes detailed program data, household outcome metrics.\n",
      "C) Collects outcome data through surveys, multiple RCTs.\n",
      "\n",
      "4) What I'm unsure about\n",
      "- None.\n",
      "\n",
      "5) Citations\n",
      "- A) [SNIPPET 1] — https://www.thelifeyoucansave.org/best-charities/raising-the-village/\n",
      "- B) [SNIPPET 2] — https://www.thelifeyoucansave.org/best-charities/raising-the-village/\n",
      "- C) [SNIPPET 3] — https://www.thelifeyoucansave.org/best-charities/village-enterprise\n"
     ]
    }
   ],
   "source": [
    "# Retriever\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a donation advisor.\n",
    "\n",
    "STRICT RULES:\n",
    "- Use ONLY the provided context snippets.\n",
    "- Never invent numbers, ratings, or organizational details.\n",
    "- If the context is insufficient, say exactly: \"I can't recommend based on my sources.\" and ask 1–2 clarifying questions.\n",
    "- Always include citations in the form: [SNIPPET X + URL].\n",
    "- Cite ONLY the snippets you actually relied on in your answer.\n",
    "- Do not add extra sections. Do not repeat the instructions.\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_FORMAT = \"\"\"Answer using EXACTLY these 5 sections and STOP after section 5.\n",
    "\n",
    "Rules:\n",
    "- Use IDs A, B, C and reuse the same IDs in all sections.\n",
    "- Max 3 charities.\n",
    "- Keep it short: each line in sections 2 and 3 must be <= 18 words.\n",
    "- No sub-bullets.\n",
    "- No numbers unless explicitly present in snippets.\n",
    "- If a section has no content, write \"None.\"\n",
    "- In section 5, copy the FULL URL exactly as shown in the context snippets.\n",
    "- Do NOT write the word \"URL\". Use the actual link.\n",
    "STOP after section 5.\n",
    "\n",
    "1) Recommended charities\n",
    "A) <Charity name>\n",
    "B) <Charity name>\n",
    "C) <Charity name>\n",
    "\n",
    "2) Why\n",
    "A) <reason>\n",
    "B) <reason>\n",
    "C) <reason>\n",
    "\n",
    "3) Transparency notes\n",
    "A) <note or \"None.\">\n",
    "B) <note or \"None.\">\n",
    "C) <note or \"None.\">\n",
    "\n",
    "4) What I'm unsure about\n",
    "- <one uncertainty>\n",
    "(or write \"None.\")\n",
    "\n",
    "5) Citations\n",
    "- A) SNIPPET X — <full link from snippet>\n",
    "- B) SNIPPET Y — <full link from snippet>\n",
    "- C) SNIPPET Z — <full link from snippet>\n",
    "\n",
    "End with <<<END>>>.\n",
    "\"\"\"\n",
    "\n",
    "def get_recommendation(query: str, min_sources: int = 2, max_snippets: int = 3):\n",
    "    nodes = retriever.retrieve(query) or []\n",
    "\n",
    "    # Si no hay suficiente información, hacer preguntas\n",
    "    if len(nodes) < min_sources:\n",
    "        return {\n",
    "            \"answer\": (\n",
    "                \"I can't recommend based on my sources. \"\n",
    "                \"Could you clarify your cause preference and whether you have any region constraints?\"\n",
    "            ),\n",
    "            \"citations\": []\n",
    "        }\n",
    "\n",
    "    context_parts = []\n",
    "    citations = []\n",
    "\n",
    "    for idx, node in enumerate(nodes[:max_snippets], start=1):\n",
    "        md = node.node.metadata or {}\n",
    "        snippet_text = node.node.get_text().strip()\n",
    "\n",
    "        # Si por lo que sea viene vacío, sáltalo\n",
    "        if not snippet_text:\n",
    "            continue\n",
    "\n",
    "        source_url = md.get(\"source_url\", \"\")\n",
    "        source_primary = md.get(\"source_primary\", \"\")\n",
    "\n",
    "        context_parts.append(\n",
    "            f\"[SNIPPET {idx}] SOURCE: {source_primary}\\n\"\n",
    "            f\"{snippet_text}\\n\"\n",
    "            f\"URL: {source_url}\\n\"\n",
    "        )\n",
    "\n",
    "        citations.append({\n",
    "            \"snippet_id\": idx,\n",
    "            \"charity_name\": md.get(\"name\", \"\"),\n",
    "            \"source_url\": source_url,\n",
    "            \"primary_source\": source_primary\n",
    "        })\n",
    "\n",
    "    # Si no tenemos suficiente contexto, hacer preguntas\n",
    "    if len(context_parts) < min_sources:\n",
    "        return {\n",
    "            \"answer\": (\n",
    "                \"I can't recommend based on my sources. \"\n",
    "                \"Could you clarify your cause preference and any region constraints?\"\n",
    "            ),\n",
    "            \"citations\": []\n",
    "        }\n",
    "\n",
    "    prompt = f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "User question:\n",
    "{query}\n",
    "\n",
    "Context snippets:\n",
    "{chr(10).join(context_parts)}\n",
    "\n",
    "{ANSWER_FORMAT}\n",
    "\n",
    "End your answer with the token <<<END>>>.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    raw = Settings.llm.complete(prompt).text\n",
    "    response = raw.split(\"<<<END>>>\")[0].strip()\n",
    "\n",
    "    used = set(int(x) for x in re.findall(r\"\\[SNIPPET\\s*(\\d+)\", response))\n",
    "    filtered = [c for c in citations if c[\"snippet_id\"] in used]\n",
    "    \n",
    "    return {\"answer\": response, \"citations\": filtered}\n",
    "\n",
    "# Ejemplo\n",
    "query = \"I have €20/month and want to reduce extreme poverty with measurable outcomes. Recommend 1-3 charities.\"\n",
    "result = get_recommendation(query)\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
