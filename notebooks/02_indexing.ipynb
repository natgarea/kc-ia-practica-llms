{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742255b4",
   "metadata": {},
   "source": [
    "# Indexación\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este notebook vamos a:\n",
    "* Cargar los datos del CSV.\n",
    "* Crear los `Documents` a partir de las filas del CSV.\n",
    "* Dividir los `Documents` en chunks y generar los embeddings de esos chunks.\n",
    "* Crear un `VectorStoreIndex` para que se pueda hacer la recuperación semántica.\n",
    "* Persistir los embeddings y metadatos en Chroma.\n",
    "\n",
    "Usaremos LlamaIndex como framework de indexación y Chroma como vectorstore.\n",
    "\n",
    "## Documentos de referencia\n",
    "* [Indexing | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/module_guides/indexing/)\n",
    "* [Storing | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/understanding/rag/storing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52278e2",
   "metadata": {},
   "source": [
    "## Imports\n",
    "* Importamos `pandas` para trabajar con el CSV.\n",
    "* De `llama_index.core` importamos:\n",
    "    * [Document](https://developers.llamaindex.ai/python/framework/module_guides/loading/documents_and_nodes/): lo usamos para representar cada fila del CSV como un objeto estructurado.\n",
    "    * [VectorStoreIndex](https://developers.llamaindex.ai/python/framework/module_guides/indexing/vector_store_index/): es la estructura que organiza los documentos y permite la recuperación semántica sobre un vectorstore.\n",
    "    * [StorageContext](https://developers.llamaindex.ai/python/framework-api-reference/storage/storage_context/): permite configurar cómo y dónde se almacenan los datos del índice.\n",
    "* De `llama_index.embeddings` nos traemos [SentenceTransformers de Hugging Face](https://huggingface.co/sentence-transformers) para convertir los chunks en vectores.\n",
    "* De `llama_index.vector_stores` importamos [ChromaVectorStore](https://developers.llamaindex.ai/python/framework/integrations/vector_stores/chroma_metadata_filter/) para integrar LlamaIndex con Chroma.\n",
    "* Importamos `chromadb` para inicializar y gestionar el cliente de Chroma, que actúa como base de datos vectorial persistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6aa5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e5b6f1-245e-4e30-b029-ce8ab20313db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64a0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear lista documents\n",
    "df = pd.read_csv(\"../data/charities.csv\")\n",
    "\n",
    "documents = []\n",
    "for _, r in df.iterrows():\n",
    "    text = f\"\"\"Name: {r['name']}\n",
    "        Cause: {r['cause']}\n",
    "        Regions: {r['regions']}\n",
    "        Primary source: {r['source_primary']}\n",
    "\n",
    "        Evidence:\n",
    "        {r['evidence_text']}\n",
    "\n",
    "        Transparency notes:\n",
    "        {str(r.get('transparency_notes', ''))} # evitamos que falle si vacío\n",
    "        \"\"\"\n",
    "    metadata = {\n",
    "                \"id\": r[\"id\"],\n",
    "                \"name\": r[\"name\"],\n",
    "                \"cause\": r[\"cause\"],\n",
    "                \"regions\": r[\"regions\"],\n",
    "                \"source_primary\": r[\"source_primary\"],\n",
    "                \"source_url\": r[\"source_url\"],\n",
    "                \"transparency_url\": r.get(\"transparency_url\", \"\"), # evitamos que falle si vacío\n",
    "                }\n",
    "    documents.append(Document(text=text, metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eeffd86-7121-4381-83f4-aac74378f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en disco\n",
    "# https://developers.llamaindex.ai/python/framework/integrations/vector_stores/chromaindexdemo/#basic-example-including-saving-to-disk\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"charities\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model,)\n",
    "\n",
    "# Persiste\n",
    "index.storage_context.persist(persist_dir=\"../data/index_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed08bd7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
